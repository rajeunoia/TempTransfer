Low-Latency Data Lake Consumption via Caching: Industry Approaches

Many organizations have published research or technical blogs on how to achieve low-latency data consumption from data lakes or data warehouses by using intermediate caches or stores, all while maintaining data freshness. A common pattern is to complement a large-scale data lake with a faster query layer (such as a data warehouse, distributed cache, or materialized view) to serve operational or interactive queries quickly. For example, AWS notes that combining a data lake with a warehouse or caches (e.g. Redis, NoSQL stores) allows teams to build daily aggregations or serve hot data with millisecond latency, rather than querying the entire raw lake for each request ￼. Below are several industry examples and their approaches to this challenge, including cloud-agnostic solutions and cloud-provider services.

Alluxio – Caching Layer for Cloud Data Lakes

Alluxio (an open-source project and company by the same name) published a whitepaper demonstrating a “meet-in-the-middle” caching layer on top of cloud object storage to accelerate data lake queries. Alluxio is deployed between the compute engines and the storage (e.g. AWS S3), caching hot dataset fragments on local SSDs or memory. This intermediate layer delivers sub-millisecond time-to-first-byte latencies and scales to very high throughput – for instance, a 50-node Alluxio cluster can serve around 1 million queries per second, achieving up to 1000× speedups over directly querying S3 ￼. Alluxio achieves these gains without requiring special hardware or changes to the underlying data format. It balances low latency, scalability, and cost by caching “hot” portions of Parquet files and even offloading operations (like predicate pushdown) closer to the data ￼. This design gives it cloud-agnostic flexibility: Alluxio can sit on any cloud or on-premise, boosting performance of data lake queries while the source data remains in cheaper storage. (Notably, companies like Salesforce have collaborated on this solution, and Alluxio is used with engines like Presto/Trino in such scenarios.)

Uber – Fresh Caching for Presto (Alluxio Local Cache)

Uber Engineering faced similar challenges at petabyte scale and wrote about speeding up Presto (their interactive SQL engine) using Alluxio as a local cache on each Presto worker node. By caching frequently accessed HDFS/S3 data on local NVMe SSDs, Uber drastically improved query latency for dashboards and ad-hoc queries ￼. A key insight from Uber’s experience was the importance of data freshness in caching. Uber’s data lake tables (often in Apache Hudi format) are updated continuously via upserts, so cached partitions can become stale. Initially, Uber found that using only a file path as the cache key led to inconsistent results when the underlying data changed ￼. Their solution was to incorporate the last modified timestamp of each partition into the cache key. In effect, if a table partition was updated, the cache would treat it as a new entry and fetch fresh data on the next query. This ensured users always saw up-to-date results from Presto, with the cache automatically invalidating old data when a newer version exists ￼. Uber noted this approach keeps query results consistent with the source data (aside from an extremely narrow race condition) and avoids serving stale data, at the cost of some extra cache space for the updated partitions ￼. This illustrates how an intermediate cache can be used for low-latency analytics without sacrificing data freshness, by designing proper invalidation logic.

LinkedIn – Venice Derived Data Store for Low-Latency Serving

LinkedIn has published about Venice, its in-house derived data storage platform, which was open-sourced in 2022. Venice is essentially a specialized intermediate store used to serve pre-computed data (features, recommendation data, etc.) with very low latency, while the bulk of data processing happens in offline/batch systems. According to LinkedIn, “Venice is a high-throughput, low-latency, highly-available, horizontally-scalable, eventually-consistent storage system with first-class support for ingesting the output of batch and stream processing jobs.” ￼ In practice, LinkedIn’s data lake (Hadoop-based) or streaming pipelines compute derived datasets (such as ML features or aggregated metrics) and push those results into Venice. Venice stores them as key-value data and serves online queries (e.g. for machine learning models or user-facing features) with millisecond response times. To maintain freshness, Venice follows an asynchronous update model: all writes come from offline/nearline jobs rather than user transactions. This means data in Venice is updated via batch pushes or stream ingestion, and it forgoes strong consistency on individual writes in favor of eventual consistency and throughput ￼. This design (similar to Apache Pinot’s approach) allows extremely fast reads and high write throughput, since Venice doesn’t handle interactive OLTP writes. Over the years, Venice has scaled to replace older caching systems (like Voldemort) at LinkedIn and now powers thousands of datasets for AI products (e.g. “People You May Know” recommendations) ￼ ￼. In summary, LinkedIn’s Venice is a cloud-agnostic solution that caches derived data from a data lake environment into an operational data store, optimizing for low-latency consumption while accepting a slight delay (asynchronous propagation) in updates.

Dremio – Data Reflections (Materialized Caching on the Lakehouse)

Dremio, a company offering a data lakehouse query engine, has introduced a feature called Data Reflections to achieve high performance on data lake storage without sacrificing freshness. A Reflection in Dremio is essentially an automatically managed materialized view (or cached subset) of data that the query engine can leverage transparently. Dremio’s reflections can take different forms – for example, a raw reflection might be a physically optimized copy of a portion of a dataset (certain columns or filtered rows), and an aggregation reflection might precompute a heavy group-by query result. These are stored as optimized Parquet files with additional indexing metadata. When a user query can be answered by a Reflection, Dremio’s optimizer will substitute the reflection in place of the base table to avoid scanning the entire data lake each time. In their blog, Dremio explains that reflections let the system “avoid repeatedly running the same transformations and computations by effectively caching these optimized snapshots,” thereby enabling much faster analytics with minimal load on the original data sources ￼. Importantly, Data Reflections are kept in sync with the source data: Dremio can incrementally update reflections when the underlying Apache Iceberg tables or other sources change, ensuring the cached views remain fresh (or they can be fully refreshed on a schedule, depending on use case). This approach provides warehouse-like query performance directly on cloud data lake storage. For instance, one can define a reflection that pre-aggregates last week’s transactions; as new data arrives, the reflection is updated so that BI dashboards always get fast responses on up-to-date data, without manually rebuilding caches. Dremio’s use of an open table format (Iceberg) and standard storage means this solution is not tied to a specific cloud vendor, aligning with a cloud-agnostic lakehouse architecture.


Sources:
	•	AWS Whitepaper – “Lake House Architecture” (data lake + warehouse/caching best practices) ￼
 
	•	Alluxio Whitepaper – “1,000× Performance Boost Querying Parquet on Cloud Data Lakes” ￼ 
 ￼https://www.alluxio.io/whitepaper/meet-in-the-middle-for-a-1-000x-performance-boost-querying-parquet-files-on-petabyte-scale-data-lakes#:~:text=We%20introduce%20Alluxio%20as%20a,instead%20of%20the%20entire%20datalake
	•	Uber Engineering Blog – “Speed Up Presto at Uber with Alluxio Local Cache” (freshness via cache invalidation)
 ￼ ￼https://www.uber.com/en-ES/blog/speed-up-presto-with-alluxio-local-cache/#:~:text=The%20challenge%20is%20that%20the,resulting%20in%20an%20inconsistent%20experience
 
	•	LinkedIn Engineering Blog – “Open Sourcing Venice: LinkedIn’s Derived Data Platform” ￼ ￼
 https://www.linkedin.com/blog/engineering/open-source/open-sourcing-venice-linkedin-s-derived-data-platform#:~:text=We%20are%20proud%20to%20announce,batch%20and%20stream%20processing%20jobs
 
	•	Dremio Blog – “How Dremio’s Reflections Enhance Iceberg Lakehouses” (materialized caching on lakehouse) ￼ ￼
 https://www.dremio.com/blog/iceberg-lakehouses-and-dremio-reflections/#:~:text=to%20the%20headaches%20of%20materialized,caching%20these%20optimized%20snapshots%2C%20enabling
 
￼
 
